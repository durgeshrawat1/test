import os
import json
import boto3
import pymongo
import numpy as np
from sklearn.metrics.pairwise import cosine_similarity

# Initialize AWS clients
bedrock_runtime = boto3.client("bedrock-runtime", region_name=os.getenv("AWS_REGION"))
docdb_client = pymongo.MongoClient(os.getenv("MONGODB_URI"))
db = docdb_client["your_database"]
collection = db["your_collection"]

# === FUNCTIONS ===

def generate_embedding(text: str) -> list:
    """Generate embedding using AWS Bedrock's Titan model (2025 version)."""
    payload = {
        "inputText": text,
        "dimensions": 1024,  # Use 1024 dimensions for Titan embeddings
        "normalize": True
    }

    response = bedrock_runtime.invoke_model(
        modelId="amazon.titan-embed-text-v2:2025",  # Updated model ID for 2025
        body=json.dumps(payload),
        contentType="application/json",
        accept="application/json"
    )

    return json.loads(response['body'].read())["embedding"]

def search_similar_documents(query_embedding: list, k: int = 3):
    """Search for similar documents by calculating cosine similarity with DocumentDB embeddings."""
    # Fetch embeddings from DocumentDB
    cursor = collection.find({}, {"_id": 0, "text": 1, "embedding": 1})  # Adjust this to include embeddings
    document_embeddings = []
    documents = []
    
    for doc in cursor:
        documents.append(doc)
        document_embeddings.append(doc["embedding"])  # Assuming embeddings are stored in 'embedding' field

    # Convert the list of document embeddings to a numpy array
    document_embeddings = np.array(document_embeddings)

    # Calculate cosine similarity between the query embedding and document embeddings
    similarities = cosine_similarity([query_embedding], document_embeddings)[0]

    # Sort documents based on similarity score and get the top k
    top_k_indices = similarities.argsort()[-k:][::-1]

    # Return the top k documents
    return [documents[i] for i in top_k_indices]

def generate_response(prompt: str):
    """Generate a response using AWS Bedrock's Claude model (2025 version)."""
    response = bedrock_runtime.invoke_model(
        modelId="anthropic.claude-3-sonnet-2025-v1:0",  # Updated model ID for 2025 Claude Sonnet
        body=json.dumps({"messages": [{"role": "user", "content": prompt}]}),
        contentType="application/json",
        accept="application/json"
    )

    result = json.loads(response['body'].read())
    return result["content"][0]["text"]

# === MAIN INTERACTION LOOP ===

def main():
    print("RAG with AWS Bedrock and DocumentDB")
    print("Type 'exit' to quit.\n")

    while True:
        user_input = input("You: ").strip()
        
        # Exit if the user types 'exit'
        if user_input.lower() == 'exit':
            break

        print("→ Generating embedding for query...")
        # Step 1: Generate embedding for the user's query
        query_embedding = generate_embedding(user_input)

        print("→ Searching DocumentDB for relevant documents...")
        # Step 2: Search for similar documents using manual cosine similarity
        documents = search_similar_documents(query_embedding)

        if not documents:
            print("No relevant documents found in the database.")
            continue

        # Step 3: Build context for the Claude model
        context = "\n\n".join([doc["text"] for doc in documents])
        prompt = f"""Use the following context to answer the question.

Context:
{context}

Question: {user_input}"""

        print("→ Querying Claude for a response...")
        # Step 4: Generate a response using Claude model
        response = generate_response(prompt)

        print("\nClaude:", response)
        print("-" * 80)

if __name__ == "__main__":
    main()
