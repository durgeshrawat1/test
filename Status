wanted to share a quick update on the work completed so far for the Accelerated Intelligent Document Processing on AWS solution (GitHub link).

Work Completed:
	1.	Terraform Conversion:
The entire codebase has been successfully converted to Terraform for easier integration with our infrastructure-as-code practices.
	2.	Compliance Review:
Since certain AWS services like Serverless EMR are not approved in our organization, I reviewed the architecture and removed all unsupported resources from the codebase.
	3.	Enablement of Approved Services:
For services that are approved but not yet enabled in our environment, I worked on enabling them in the lab setup as part of the preparation.
	4.	Deployment:
I’ve completed the deployment for Pattern 1 of the solution. However, testing could not be performed due to the use of AWS AppSync, which is not currently approved in our organization.

Please let me know if you’d like a walkthrough of the Terraform implementation or if we should explore alternatives to AppSync for testing and production readiness.



✅ Work Completed So Far:
	1.	Terraform Conversion:
The complete solution codebase has been converted to Terraform, aligning with our infrastructure-as-code standards.
	2.	Service Compliance Review:
Removed components that rely on services not approved in our organization (e.g., Serverless EMR).
For services that are approved but not yet enabled, I worked on enabling them in the lab environment.
	3.	Initial Deployment:
Deployed the solution for Pattern 1. However, AppSync, which is a dependency in this pattern, is not approved in our environment—so we were unable to run end-to-end test cases.
	4.	Lambda Cleanup & Dependency Handling:
	•	Several Lambda functions in the repo lacked proper requirements.txt definitions, leading to dependency issues.
	•	We are currently cleaning up and restructuring the Lambda code to properly define and install dependencies.
	•	A few Lambda functions exceeded the size limit due to large dependencies; for those, we are creating and integrating Lambda layers to manage package size constraints.

# -------- Boto3: Invoke Model --------
import boto3
import json

# Use assumed role credentials here if needed
bedrock_runtime = boto3.client('bedrock-runtime', region_name='us-east-1')

invoke_response = bedrock_runtime.invoke_model(
    modelId='anthropic.claude-v2',
    contentType='application/json',
    accept='application/json',
    body=json.dumps({
        "prompt": "\n\nHuman: What is the capital of France?\n\nAssistant:",
        "max_tokens_to_sample": 100,
        "temperature": 0.7
    })
)

print("InvokeModel response:")
print(invoke_response['body'].read().decode())

# -------- Boto3: List Guardrails --------
bedrock_control = boto3.client('bedrock', region_name='us-east-1')

list_response = bedrock_control.list_guardrails()
print("ListGuardrails response:")
print(list_response['guardrails'])

# -------- Boto3: Get Guardrail --------
get_response = bedrock_control.get_guardrail(guardrailIdentifier='your-guardrail-id')
print("GetGuardrail response:")
print(get_response)

# -------- LangChain: Invoke Bedrock Model --------
from langchain_community.llms import Bedrock

llm = Bedrock(
    region_name="us-east-1",
    model_id="anthropic.claude-v2",
    model_kwargs={
        "temperature": 0.5,
        "max_tokens_to_sample": 100
    }
)

langchain_response = llm.invoke("What is the capital of France?")
print("LangChain response:")
print(langchain_response)
