import polars as pl
import aiohttp
import asyncio
import time
import json
import os
import sys
import toml
import logging
import shutil
from urllib.parse import urlencode
from datetime import datetime
import random


def sanitize(text: str) -> str:
    return text.replace('\n', ' ').replace('\r', ' ').replace('\x00', '')


def load_config(path='config.toml'):
    if not os.path.exists(path):
        raise FileNotFoundError(f"Config file {path} not found.")
    return toml.load(path)


def archive_existing_log(log_file: str):
    if os.path.exists(log_file):
        archive_folder = os.path.join(os.path.dirname(log_file), 'archive_logs')
        os.makedirs(archive_folder, exist_ok=True)
        timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
        archived_name = f"log_{timestamp}.log"
        shutil.move(log_file, os.path.join(archive_folder, archived_name))


def setup_logger(log_file: str):
    archive_existing_log(log_file)
    os.makedirs(os.path.dirname(log_file), exist_ok=True)
    logging.basicConfig(
        filename=log_file,
        filemode='a',
        level=logging.INFO,
        format='%(asctime)s | %(levelname)s | %(message)s'
    )
    logging.info("üîß Logging initialized.")


GOOGLE_RATE_LIMIT_PER_SECOND = 10  # max 10 elements/sec according to Google docs
MAX_RETRIES = 5


async def fetch_single_distance(session, rec, api_key, retry=0):
    await rate_limiter.acquire()
    try:
        params = {
            'origins': rec['source_address'],
            'destinations': rec['destination_address'],
            'key': api_key
        }
        url = 'https://maps.googleapis.com/maps/api/distancematrix/json?' + urlencode(params)
        timestamp = time.strftime('%Y-%m-%d %H:%M:%S')

        async with session.get(url) as response:
            if response.status == 429:
                if retry < MAX_RETRIES:
                    wait_time = 2 ** retry + random.uniform(0.1, 0.5)
                    logging.warning(f"üïí 429 Too Many Requests ‚Äî retrying in {wait_time:.1f}s (attempt {retry+1})")
                    await asyncio.sleep(wait_time)
                    return await fetch_single_distance(session, rec, api_key, retry=retry+1)
                else:
                    logging.error("‚ùå Max retries reached for rate limiting.")
                    return None

            if response.status != 200:
                logging.error(f"‚ùå API returned status {response.status} for employee {sanitize(rec['employee'])}")
                return None

            result = await response.json()

        try:
            element = result['rows'][0]['elements'][0]
            distance = element.get('distance', {})
            duration = element.get('duration', {})
        except Exception as e:
            distance, duration = {}, {}
            logging.warning(f"‚ö†Ô∏è Missing data for {sanitize(rec['employee'])}: {sanitize(str(e))}")

        return {
            'employee': rec['employee'],
            'source': rec['source_address'],
            'destination': rec['destination_address'],
            'distance_text': distance.get('text', ''),
            'distance_value': distance.get('value', ''),
            'duration_text': duration.get('text', ''),
            'duration_value': duration.get('value', ''),
            'timestamp': timestamp,
            'raw_response': json.dumps(result)
        }

    finally:
        # Enforce rate limit delay between acquiring permits
        await asyncio.sleep(1 / GOOGLE_RATE_LIMIT_PER_SECOND)
        rate_limiter.release()


async def fetch_all_distances(records, api_key, concurrency=10):
    semaphore = asyncio.Semaphore(concurrency)
    global rate_limiter
    rate_limiter = asyncio.Semaphore(GOOGLE_RATE_LIMIT_PER_SECOND)

    async with aiohttp.ClientSession() as session:
        async def safe_fetch(rec):
            async with semaphore:
                return await fetch_single_distance(session, rec, api_key)

        results = await asyncio.gather(*(safe_fetch(r) for r in records))
        return [r for r in results if r is not None]


async def main():
    config = load_config()
    log_file = config['files']['log_file']
    setup_logger(log_file)

    input_dir = config['files']['input_dir']
    output_path = config['files']['output_path']
    api_key = config['google_api']['api_key']

    source_cols = config['address_columns']['source']
    destination_cols = config['address_columns']['destination']

    if len(sys.argv) < 2:
        logging.error("‚ùå Missing input filename argument.")
        print("Usage: python script.py <input_filename>")
        sys.exit(1)

    input_filename = sys.argv[1]
    input_path = os.path.join(input_dir, input_filename)

    logging.info(f"üì• Reading input file: {sanitize(input_path)}")
    logging.info(f"üì§ Output will be saved to: {sanitize(output_path)}")

    try:
        df = pl.read_csv(input_path, separator='|')
    except Exception as e:
        logging.error(f"‚ùå Failed to read input: {sanitize(str(e))}")
        sys.exit(1)

    df = df.with_columns([
        pl.concat_str(source_cols, separator=' ').alias('source_address'),
        pl.concat_str(destination_cols, separator=' ').alias('destination_address'),
    ])

    data = df.select(['employee', 'source_address', 'destination_address']).to_dicts()

    logging.info(f"üöÄ Starting fetch for {len(data)} records with concurrency=10 and rate limit=10 elem/sec")

    all_results = await fetch_all_distances(data, api_key, concurrency=10)

    if not all_results:
        logging.warning("‚ö†Ô∏è No results returned. Nothing to write.")
    else:
        os.makedirs(os.path.dirname(output_path), exist_ok=True)
        pl.DataFrame(all_results).write_csv(output_path, separator='|')
        logging.info(f"‚úÖ Output written to {sanitize(output_path)}")

    # Delete input file after successful processing
    try:
        os.remove(input_path)
        logging.info(f"üóëÔ∏è Deleted input file: {sanitize(input_path)}")
    except Exception as e:
        logging.warning(f"‚ö†Ô∏è Failed to delete input file: {sanitize(str(e))}")

    logging.info("üèÅ Script finished.")


if __name__ == '__main__':
    start = time.time()
    try:
        asyncio.run(main())
    except Exception as e:
        logging.critical(f"üí• Uncaught exception: {sanitize(str(e))}")
        raise
    print(f"‚è±Ô∏è Done in {time.time() - start:.2f} seconds")
